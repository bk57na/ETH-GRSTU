{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet, CrossEntropy\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def add_features(raw_returns: pd.Series) -> pd.DataFrame:\n",
    "    features = {}\n",
    "    hls = [5, 20, 60]\n",
    "\n",
    "    for hl in hls:\n",
    "        # Feature 1: EWM-ret\n",
    "        features[f'ret_{hl}'] = raw_returns.ewm(halflife=hl).mean()\n",
    "        # Feature 2: log(EWM-DD)\n",
    "        sq_mean = np.minimum(raw_returns, 0.).pow(2).ewm(halflife=hl).mean()\n",
    "        dd = np.sqrt(sq_mean)\n",
    "        features[f'dd-log_{hl}'] = np.log(dd)\n",
    "        # Feature 3: EWM-Sortino-ratio = EWM-ret/EWM-DD\n",
    "        features[f'sortino_{hl}'] = features[f'ret_{hl}'].div(dd)\n",
    "\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def identify_regimes(df):\n",
    "    # Create regime-specific features\n",
    "    scaled_features = StandardScaler().fit_transform(df)\n",
    "\n",
    "    # Apply Gaussian Mixture Model\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    regimes = gmm.fit_predict(scaled_features)\n",
    "\n",
    "    # Ensure we have the right length\n",
    "    full_regimes = np.full(len(df), -1)\n",
    "    start_idx = len(df) - len(regimes)\n",
    "    full_regimes[start_idx:] = regimes\n",
    "\n",
    "    return full_regimes\n",
    "\n",
    "\n",
    "df = pd.read_csv('sp500.csv', parse_dates=['date'])\n",
    "returns = df['returns']\n",
    "data = add_features(returns)\n",
    "data['returns'] = returns\n",
    "data['regime'] = identify_regimes(data).astype(str)\n",
    "data['date'] = df['date']\n",
    "data['price'] = df['close']\n",
    "data['fin_type'] = 'sp500'\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data.index\n",
    "data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "max_prediction_length = 30\n",
    "max_encoder_length = 24\n",
    "training_cutoff = 5808\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[:5808],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"regime\",\n",
    "    group_ids=[\"fin_type\"],\n",
    "    min_encoder_length=max_encoder_length // 2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"fin_type\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"returns\"],\n",
    "    time_varying_unknown_categoricals=['regime'],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"ret_5\",\n",
    "        \"dd-log_5\",\n",
    "        \"sortino_5\",\n",
    "        \"ret_20\",\n",
    "        \"dd-log_20\",\n",
    "        \"sortino_20\",\n",
    "        \"ret_60\",\n",
    "        \"dd-log_60\",\n",
    "        \"sortino_60\",\n",
    "    ],\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create a validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"ranger\",\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.0158,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=CrossEntropy(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions = tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"), mode=\"raw\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_predictions = []\n",
    "all_predictions_smoothed = []\n",
    "\n",
    "for t in range(training_cutoff, 11010, 30):\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training, data[:t], predict=True, stop_randomization=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = validation.to_dataloader(\n",
    "        train=False, batch_size=batch_size * 10, num_workers=0\n",
    "    )\n",
    "\n",
    "    pred = tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\")).y[0]\n",
    "    all_predictions.append(pred)\n",
    "    all_predictions_smoothed.append(1 if pred.sum() > 15 else 0)\n",
    "\n",
    "predictions = np.array(all_predictions).flatten()\n",
    "predictions_smoothed = np.array(all_predictions_smoothed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evaluation import plot_regimes, evaluate_strategy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "regimes_plotted = np.array(all_predictions_smoothed).repeat(30)\n",
    "\n",
    "plot_regimes(df['date'].values[training_cutoff:11012], df['close'].values[training_cutoff:11012], regimes_plotted[16:11012 - training_cutoff + 16])\n",
    "\n",
    "for d in range(0, 17):\n",
    "    print('Delta ' + str(d))\n",
    "    evaluate_strategy(df['returns'].values[training_cutoff:11012], regimes_plotted[d:11012 - training_cutoff + d], 1)\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "\n",
    "sns.set_theme(context=\"paper\", style='whitegrid')\n",
    "\n",
    "evaluate_strategy(df['returns'].values[training_cutoff:11012], regimes_plotted[d:11012 - training_cutoff + d], 1)\n",
    "\n",
    "evaluate_strategy(df['returns'].values[training_cutoff:11012 ], regimes_plotted[:11012 - training_cutoff], 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_strategy(df['returns'].values[training_cutoff - delta:11028 - delta], all_predictions, 1)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
