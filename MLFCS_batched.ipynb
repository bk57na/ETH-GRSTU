{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_theme(context='paper')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Using {device} device')\n",
    "\n",
    "K = 2  # Number of hidden states\n",
    "\n",
    "# Hyperparameters for 2 states (/3 states), defined in exhibit 2\n",
    "H = 32  # GRSTU hidden state size\n",
    "grad_clip = 0.001  # Gradient clipping threshold\n",
    "B = 64  # Batch size\n",
    "R = 20  # (/80) Length of truncated backpropagation\n",
    "L = 30  # Window of past observations fed to the model\n",
    "J = 10  # Parameter for the likelihood loss\n",
    "nu = 0.005  # Learning rate\n",
    "U = 30  # Frequency at which the distributions are updated\n",
    "C_1 = 0.1\n",
    "E_1 = 500  # (/400)\n",
    "E_2 = 250  # (/250)\n",
    "epochs = 750  # Number of epochs for which the model is trained\n",
    "p = 7\n",
    "lambda_1 = 3  # Jump penalty\n",
    "\n",
    "t_min = L + 10  # Minimum time step for evaluation"
   ],
   "id": "5351dc5bc902b903",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a302da4f8976857a",
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gaussian_log_likelihood(x: torch.Tensor, mean: torch.Tensor, var: torch.Tensor) -> torch.Tensor:\n",
    "    # Equation 9\n",
    "    return -0.5 * torch.log(2 * np.pi * var) - 0.5 * ((x - mean) ** 2) / var\n",
    "\n",
    "\n",
    "def gaussian_kl_divergence(mean1, var1, mean2, var2):\n",
    "    # Equation 16\n",
    "    return 0.5 * (torch.log(var2 / var1) + var1 / var2 + ((mean1 - mean2) ** 2) / var2 - 1)\n",
    "\n",
    "\n",
    "class StraightThroughEstimator(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Returns one hot-hot encoded vector (definition of s_t in bottom of page 5)\n",
    "        one_hot = torch.zeros_like(x)\n",
    "        out = one_hot.scatter_(1, x.argmax(dim=1).unsqueeze(1), 1.0)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Transparent backward pass\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "class GRSTU(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.W_u = nn.Parameter(torch.empty(input_size, hidden_size))\n",
    "        self.W_r = nn.Parameter(torch.empty(input_size, hidden_size))\n",
    "        self.W_h = nn.Parameter(torch.empty(input_size, hidden_size))\n",
    "        self.W_y = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
    "        self.W_p = nn.Parameter(torch.empty(hidden_size, output_size))\n",
    "        self.R_u = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
    "        self.R_r = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
    "        self.R_h = nn.Parameter(torch.empty(hidden_size, hidden_size))\n",
    "\n",
    "        self.b_u = nn.Parameter(torch.empty(hidden_size))\n",
    "        self.b_r = nn.Parameter(torch.empty(hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.empty(hidden_size))\n",
    "        self.b_y = nn.Parameter(torch.empty(hidden_size))\n",
    "        self.b_p = nn.Parameter(torch.empty(output_size))\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.hidden = None\n",
    "\n",
    "        self.ste = StraightThroughEstimator.apply\n",
    "\n",
    "        # Same initialization as PyTorch GRU\n",
    "        stdv = 1.0 / math.sqrt(hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def init_hidden(self, batch_size: int = 1, h_device=None) -> None:\n",
    "        self.hidden = torch.zeros(batch_size, self.hidden_size, device=h_device)\n",
    "\n",
    "    def reset_hidden(self) -> None:\n",
    "        self.hidden = None\n",
    "\n",
    "    def compute_state_probabilities(self) -> torch.Tensor:\n",
    "        # Equation 7\n",
    "        relu = torch.relu(torch.matmul(self.hidden, self.W_y) + self.b_y)\n",
    "        return torch.softmax(torch.matmul(relu, self.W_p) + self.b_p, dim=-1)\n",
    "\n",
    "    def forward(self, x_t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = x_t.size(0)\n",
    "\n",
    "        if self.hidden is None:\n",
    "            self.init_hidden(batch_size, x_t.device)\n",
    "\n",
    "        # Equation 3\n",
    "        z_t = torch.sigmoid(x_t @ self.W_u + self.hidden @ self.R_u + self.b_u)\n",
    "\n",
    "        # Equation 4\n",
    "        norm_in = x_t @ self.W_r + self.hidden @ self.R_r + self.b_r\n",
    "        r_t = torch.sigmoid(self.batch_norm(norm_in) if batch_size > 1 else norm_in)\n",
    "\n",
    "        # Equation 5\n",
    "        h_dot = torch.tanh(x_t @ self.W_h + (r_t * self.hidden) @ self.R_h + self.b_h)\n",
    "\n",
    "        # Equation 6\n",
    "        self.hidden = (1 - z_t) * self.hidden + z_t * h_dot\n",
    "\n",
    "        # Equation 7\n",
    "        p_t = self.compute_state_probabilities()\n",
    "\n",
    "        # Equation 8\n",
    "        s_t = self.ste(p_t)\n",
    "\n",
    "        return s_t, p_t\n",
    "\n",
    "    def eval_predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.reset_hidden()\n",
    "\n",
    "        data_length = x.size(1)\n",
    "        out = torch.zeros(data_length - t_min, K)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t_pred in range(t_min, data_length):\n",
    "                out[t_pred - t_min] = self(x[:, t_pred - self.input_size:t_pred])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class GRSTULoss(nn.Module):\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        super().__init__()\n",
    "\n",
    "        # Equation 15 for e=0\n",
    "        self.jump_penalty = lambda_1 * np.exp(-p)\n",
    "\n",
    "        self.register_buffer('means', torch.full((K,), data.mean()))\n",
    "        self.register_buffer('vars', torch.full((K,), data.var()))\n",
    "\n",
    "    def update_gaussian_params(self, data: torch.Tensor, e: int, s_t: torch.Tensor):\n",
    "        # Equation 15\n",
    "        self.jump_penalty = lambda_1 * np.exp((p * e) / E_2 - p) if e < E_2 else lambda_1\n",
    "\n",
    "        new_means = self.means.clone()\n",
    "        new_vars = self.vars.clone()\n",
    "\n",
    "        k_t = torch.argmax(s_t, dim=1)\n",
    "        window_data = data[-s_t.size(0):]\n",
    "\n",
    "        for i in range(K):\n",
    "            N_i = torch.sum(k_t == i)\n",
    "\n",
    "            if N_i > 1:\n",
    "                # Equation 10\n",
    "                new_means[i] = torch.mean(window_data[k_t == i])\n",
    "                # Equation 11\n",
    "                new_vars[i] = torch.var(window_data[k_t == i], unbiased=True)\n",
    "            elif N_i == 1:\n",
    "                # Update mean only when single point\n",
    "                new_means[i] = torch.mean(window_data[k_t == i])\n",
    "\n",
    "        # Apply permutation prevention\n",
    "        self._prevent_label_permutation(self.means.clone(), self.vars.clone(), new_means, new_vars)\n",
    "\n",
    "    def _prevent_label_permutation(self, old_means, old_vars, new_means, new_vars) -> None:\n",
    "        distributions = list(range(K))\n",
    "\n",
    "        for i in range(K):\n",
    "            min_kl = np.inf\n",
    "            best_j = i\n",
    "\n",
    "            # Find the old distribution that minimizes KL divergence\n",
    "            for j in distributions:\n",
    "                kl = gaussian_kl_divergence(old_means[i], old_vars[i], new_means[j], new_vars[j])\n",
    "\n",
    "                if kl < min_kl:\n",
    "                    min_kl = kl\n",
    "                    best_j = j\n",
    "\n",
    "            # Ensure that this distribution is not used again\n",
    "            distributions.remove(best_j)\n",
    "\n",
    "            self.means[i] = new_means[best_j]\n",
    "            self.vars[i] = new_vars[best_j]\n",
    "\n",
    "    def forward(self, e: int, v_t: torch.Tensor, s_t: torch.Tensor, p_t: torch.Tensor,\n",
    "                p_prev: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        if p_prev is None:\n",
    "            p_prev = p_t\n",
    "\n",
    "        mean = (s_t @ self.means).unsqueeze(1)\n",
    "        var = (s_t @ self.vars).unsqueeze(1)\n",
    "\n",
    "        log_likelihood = gaussian_log_likelihood(v_t[:, -J:], mean, var)\n",
    "\n",
    "        # Equation 13\n",
    "        entropy = -torch.sum(p_t * torch.log(torch.where(p_t > 0, p_t, 1.0)), dim=1)\n",
    "\n",
    "        # Equation 14\n",
    "        beta_e = C_1 if e < E_1 else 0.0\n",
    "\n",
    "        # Equation 12\n",
    "        jump_loss = self.jump_penalty * torch.norm(p_t - p_prev.detach(), p=1, dim=1)\n",
    "        return -log_likelihood.mean(dim=1) - beta_e * entropy + jump_loss\n",
    "\n",
    "\n",
    "class TimeSeriesWindowDataset(Dataset):\n",
    "    def __init__(self, time_series: torch.Tensor, window_size: int, start: int):\n",
    "        self.time_series = time_series\n",
    "        self.window_size = window_size\n",
    "        self.t_min = start\n",
    "        self.t_max = len(time_series)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t_max - self.t_min\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = idx + self.t_min\n",
    "\n",
    "        return self.time_series[index - self.window_size:index]\n",
    "\n",
    "\n",
    "def train_grstu(data: np.ndarray):\n",
    "    time_series = torch.tensor(data, dtype=torch.float, device=device)\n",
    "    dataset = TimeSeriesWindowDataset(time_series, L, t_min)\n",
    "    dataloader = DataLoader(dataset, batch_size=B, shuffle=False, drop_last=True)\n",
    "\n",
    "    model = GRSTU(input_size=L, hidden_size=H, output_size=K).to(device)\n",
    "    loss_fn = GRSTULoss(data).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nu)\n",
    "\n",
    "    # Training variables\n",
    "    best_loss = np.inf\n",
    "    best_states = None\n",
    "    best_means = None\n",
    "    best_vars = None\n",
    "\n",
    "    loss_history = []\n",
    "    training_loop = tqdm(range(epochs))\n",
    "\n",
    "    for e in training_loop:\n",
    "        model.train()\n",
    "\n",
    "        epoch_losses = []\n",
    "        states = []\n",
    "        last_probs = None\n",
    "\n",
    "        # Each batch consists of consecutive v_t from `t` to `t + B`\n",
    "        for v_t in dataloader:\n",
    "            current_state, current_probs = model(v_t)\n",
    "\n",
    "            if last_probs is None:\n",
    "                last_probs = current_probs[0].detach()\n",
    "\n",
    "            # Each batch consists of B consecutive `t`, so to have the previous\n",
    "            # probabilities we add the last probabilities output from previous\n",
    "            # batch and add it before the outputs\n",
    "            prev_probs = torch.cat([last_probs.unsqueeze(0), current_probs[:-1]])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(e, v_t, current_state, current_probs, prev_probs).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            last_probs = current_probs[-1].detach()\n",
    "\n",
    "            model.hidden = model.hidden.detach()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "            states.append(current_state)\n",
    "\n",
    "        loss_history.append(np.mean(epoch_losses))\n",
    "\n",
    "        model.reset_hidden()\n",
    "\n",
    "        # Update Gaussian parameters every U epochs\n",
    "        if e % U == 0:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss_fn.update_gaussian_params(time_series, e, torch.cat(states, dim=0))\n",
    "\n",
    "                last_probs = None\n",
    "                eval_losses = []\n",
    "                eval_states = []\n",
    "\n",
    "                # Collect predictions for all data\n",
    "                for v_t in dataloader:\n",
    "                    # Forward pass\n",
    "                    current_state, current_probs = model(v_t)\n",
    "\n",
    "                    if last_probs is None:\n",
    "                        last_probs = current_probs[0].detach()\n",
    "\n",
    "                    prev_probs = torch.cat([last_probs.unsqueeze(0), current_probs[:-1]])\n",
    "\n",
    "                    loss = loss_fn(e, v_t, current_state, current_probs, prev_probs).mean()\n",
    "\n",
    "                    last_probs = current_probs[-1].detach()\n",
    "\n",
    "                    eval_losses.append(loss.item())\n",
    "\n",
    "                    eval_states.append(current_state)\n",
    "\n",
    "                model.reset_hidden()\n",
    "\n",
    "                # Save states prediction with the lowest evaluation loss\n",
    "                if np.mean(eval_losses) < best_loss:\n",
    "                    best_loss = np.mean(eval_losses)\n",
    "                    best_states = torch.cat(eval_states, dim=0).cpu().numpy()\n",
    "                    best_means = loss_fn.means.clone().cpu().numpy()\n",
    "                    best_vars = loss_fn.vars.clone().cpu().numpy()\n",
    "\n",
    "        training_loop.set_postfix(loss=np.mean(epoch_losses))\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_states': best_states,\n",
    "        'means': best_means,\n",
    "        'vars': best_vars,\n",
    "        'losses': loss_history,\n",
    "    }\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "sp500 = pd.read_csv('sp500.csv', parse_dates=['date'])\n",
    "data = sp500['returns'].values\n",
    "\n",
    "results = train_grstu(data)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.set_title('Training Loss')\n",
    "ax.plot(results['losses'], label='Training Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11afa666cf3c15ee",
   "metadata": {},
   "source": [
    "def plot_regimes(dates, prices, returns, regime_labels, means, vars):\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(14, 9), sharex=True)\n",
    "\n",
    "    ax1.plot(dates, prices, alpha=0.7, label='Price')\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.set_title('Regime Detection')\n",
    "\n",
    "    ax2.plot(dates, returns, alpha=0.7, label='Returns')\n",
    "    ax2.set_ylabel('Returns (%)')\n",
    "    ax2.set_xlabel('Date')\n",
    "\n",
    "    # Create a colormap for regimes\n",
    "    colors = ['g', 'r']\n",
    "\n",
    "    # Add colored backgrounds for each regime\n",
    "    for i in range(len(means)):\n",
    "        if not np.any(regime_labels == i):\n",
    "            continue\n",
    "\n",
    "        mask_indices = np.where(regime_labels == i)[0]\n",
    "        segments = np.split(mask_indices, np.where(np.diff(mask_indices) != 1)[0] + 1)\n",
    "        regime_label = f'Regime {i + 1}: μ={means[i]:.4f}, σ={np.sqrt(vars[i]):.4f}'\n",
    "\n",
    "        for segment in segments:\n",
    "            if len(segment) > 0:\n",
    "                start_idx = segment[0]\n",
    "                end_idx = segment[-1]\n",
    "                label = regime_label if segment is segments[0] else ''\n",
    "\n",
    "                ax1.axvspan(dates[start_idx], dates[end_idx], alpha=0.4, color=colors[i], label=label)\n",
    "                ax2.axvspan(dates[start_idx], dates[end_idx], alpha=0.4, color=colors[i])\n",
    "\n",
    "    # Add legend to both subplots\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Format x-axis\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dates = sp500['date'].values if 'date' in sp500.columns else np.arange(len(data))\n",
    "\n",
    "prices = sp500['close'].values if 'close' in sp500.columns else np.cumsum(data)\n",
    "\n",
    "regime_labels = np.argmax(results['best_states'], axis=1)\n",
    "\n",
    "# Plot results with dates\n",
    "plot_regimes(\n",
    "    dates[L:L + len(regime_labels)],\n",
    "    prices[L:L + len(regime_labels)],\n",
    "    data[L:L + len(regime_labels)],\n",
    "    regime_labels,\n",
    "    results['means'],\n",
    "    results['vars'],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "y_pred = np.argmax(results['best_states'], axis=1)\n",
    "y_test = sp500['states'].values[:len(y_pred)]\n",
    "\n",
    "print('Balanced Accuracy:', balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ],
   "id": "17bb782c57877b2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1115f636698e8de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
