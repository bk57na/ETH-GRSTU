{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_theme(context='paper')\n",
    "\n",
    "K = 2  # Number of hidden states\n",
    "\n",
    "# Hyperparameters for 2 states (/3 states), defined in exhibit 2\n",
    "H = 32  # GRSTU hidden state size\n",
    "grad_clip = 0.001  # Gradient clipping threshold\n",
    "B = 64  # Batch size\n",
    "R = 20  # (/80) Length of truncated backpropagation\n",
    "L = 30  # Window of past observations fed to the model\n",
    "J = 10  # Parameter for the likelihood loss\n",
    "nu = 0.005  # Learning rate\n",
    "U = 30  # Frequency at which the distributions are updated\n",
    "C_1 = 0.1\n",
    "E_1 = 500  # (/400)\n",
    "E_2 = 250  # (/250)\n",
    "epochs = 750  # Number of epochs for which the model is trained\n",
    "p = 7\n",
    "lambda_1 = 3"
   ],
   "id": "5351dc5bc902b903",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a302da4f8976857a",
   "metadata": {},
   "source": [
    "def gaussian_log_likelihood(x: torch.Tensor, mean: torch.Tensor, var: torch.Tensor) -> torch.Tensor:\n",
    "    # Equation 9\n",
    "    return -0.5 * torch.log(2 * np.pi * var) - 0.5 * ((x - mean) ** 2) / var\n",
    "\n",
    "\n",
    "class StraightThroughEstimator(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Returns one hot-hot encoded vector (definition of s_t in bottom of page 5)\n",
    "        one_hot = torch.zeros_like(x)\n",
    "        return one_hot.scatter_(1, x.argmax(dim=1).unsqueeze(1), 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output): # TODO check later\n",
    "        # Transparent backward pass\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "class GRSTU(nn.Module):\n",
    "    def __init__(self, init_std: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_u = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_r = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_h = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_y = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.W_p = nn.Parameter(torch.randn(K, H) * init_std)\n",
    "        self.R_u = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.R_r = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.R_h = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "\n",
    "        self.b_u = nn.Parameter(torch.zeros(H))\n",
    "        self.b_r = nn.Parameter(torch.zeros(H))\n",
    "        self.b_h = nn.Parameter(torch.zeros(H))\n",
    "        self.b_y = nn.Parameter(torch.zeros(H))\n",
    "        self.b_p = nn.Parameter(torch.zeros(K))\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(H)\n",
    "\n",
    "        self.hidden = None\n",
    "\n",
    "        self.ste = StraightThroughEstimator.apply\n",
    "\n",
    "    def init_hidden(self, batch_size: int = 1):\n",
    "        self.hidden = torch.zeros(batch_size, H)\n",
    "\n",
    "    def compute_state_probabilities(self):\n",
    "        # Equation 7\n",
    "        relu = torch.relu(torch.matmul(self.hidden, self.W_y.t()) + self.b_y)\n",
    "        return torch.softmax(torch.matmul(relu, self.W_p.t()) + self.b_p, dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        if self.hidden is None: #or self.hidden.size(0) != batch_size:\n",
    "            self.init_hidden(batch_size)\n",
    "\n",
    "        # Equation 3\n",
    "        z_t = torch.sigmoid(torch.matmul(x, self.W_u.t()) + torch.matmul(self.hidden, self.R_u.t()) + self.b_u)\n",
    "\n",
    "        # Equation 4\n",
    "        norm_in = torch.matmul(x, self.W_r.t()) + torch.matmul(self.hidden, self.R_r.t()) + self.b_r\n",
    "        r_t = torch.sigmoid(self.batch_norm(norm_in) if batch_size > 1 else norm_in)\n",
    "\n",
    "        # Equation 5\n",
    "        h_dot = torch.tanh(torch.matmul(x, self.W_h.t()) + torch.matmul(r_t * self.hidden, self.R_h.t()) + self.b_h)\n",
    "\n",
    "        # Equation 6\n",
    "        self.hidden = (1 - z_t) * self.hidden + z_t * h_dot\n",
    "\n",
    "        # Equation 7\n",
    "        p_t = self.compute_state_probabilities()\n",
    "\n",
    "        # Equation 8\n",
    "        s_t = self.ste(p_t)\n",
    "\n",
    "        return p_t, s_t\n",
    "\n",
    "\n",
    "class GRSTULoss(nn.Module):\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer('means', torch.full((K,), data.mean()))\n",
    "        self.register_buffer('vars', torch.full((K,), data.var()))\n",
    "        self.register_buffer('lambda_epochs', torch.zeros(epochs))\n",
    "\n",
    "    def update_gaussian_params(self, data: torch.Tensor, s_t: torch.Tensor):\n",
    "        old_means = self.means.clone()\n",
    "        old_vars = self.vars.clone()\n",
    "\n",
    "        k_t = torch.argmax(s_t, dim=1)\n",
    "\n",
    "        # Count states for debugging\n",
    "        state_counts = torch.bincount(k_t, minlength=K)\n",
    "        print(f\"State distribution: {state_counts.tolist()} out of {len(k_t)} points\")\n",
    "\n",
    "        # Check state probabilities - are they nearly binary?\n",
    "        probs_max = torch.max(s_t, dim=1)[0]\n",
    "        nearly_binary = (probs_max > 0.99).float().mean()\n",
    "        print(f\"Proportion of nearly-binary state assignments: {nearly_binary:.4f}\")\n",
    "\n",
    "        # Track parameter changes\n",
    "        old_params = {i: (self.means[i].item(), self.vars[i].item()) for i in range(K)}\n",
    "\n",
    "        for i in range(K):\n",
    "            N_i = torch.sum(k_t == i)\n",
    "\n",
    "            if N_i > 1:\n",
    "                # Equation 10\n",
    "                self.means[i] = torch.mean(data[k_t == i])\n",
    "                # Equation 11\n",
    "                self.vars[i] = torch.var(data[k_t == i], unbiased=True)\n",
    "            elif N_i == 1:\n",
    "                # Update mean only when single point\n",
    "                self.means[i] = torch.mean(data[k_t == i])\n",
    "\n",
    "        # Track parameter changes after updates\n",
    "        new_params = {i: (self.means[i].item(), self.vars[i].item()) for i in range(K)}\n",
    "\n",
    "        #print(\"Parameter changes:\")\n",
    "        for i in range(K):\n",
    "            old_mean, old_var = old_params[i]\n",
    "            new_mean, new_var = new_params[i]\n",
    "            #print(f\"State {i}: mean {old_mean:.6f} → {new_mean:.6f}, var {old_var:.6f} → {new_var:.6f}\")\n",
    "\n",
    "        # Apply permutation prevention\n",
    "        self._prevent_label_permutation(old_means, old_vars)\n",
    "\n",
    "        # Check if permutation occurred\n",
    "        post_perm_params = {i: (self.means[i].item(), self.vars[i].item()) for i in range(K)}\n",
    "        #if any(post_perm_params[i] != new_params[i] for i in range(K)):\n",
    "        #    print(\"Label permutation occurred!\")\n",
    "\n",
    "    def _prevent_label_permutation(self, old_means: torch.Tensor, old_vars: torch.Tensor):\n",
    "        for i in range(K):\n",
    "            min_kl = np.inf\n",
    "            best_j = i\n",
    "\n",
    "            # Find the old distribution that minimizes KL divergence\n",
    "            for j in range(K):\n",
    "                # KL divergence between two Gaussian\n",
    "                kl = 0.5 * (\n",
    "                        torch.log(old_vars[j] / self.vars[i]) +\n",
    "                        self.vars[i] / old_vars[j] +\n",
    "                        ((self.means[i] - old_means[j]) ** 2) / old_vars[j] -\n",
    "                        1\n",
    "                )\n",
    "\n",
    "                if kl < min_kl:\n",
    "                    min_kl = kl\n",
    "                    best_j = j\n",
    "\n",
    "            # If best match is different from current index, swap parameters\n",
    "            if best_j != i:\n",
    "                # Swap means\n",
    "                temp_mean = self.means[i].clone()\n",
    "                self.means[i] = self.means[best_j]\n",
    "                self.means[best_j] = temp_mean\n",
    "\n",
    "                # Swap variances\n",
    "                temp_var = self.vars[i].clone()\n",
    "                self.vars[i] = self.vars[best_j]\n",
    "                self.vars[best_j] = temp_var\n",
    "\n",
    "    def forward(self, e: int, current_x: torch.Tensor, p_t: torch.Tensor,\n",
    "                s_t: torch.Tensor, p_prev: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        #if p_prev is None:\n",
    "        p_prev = p_t\n",
    "\n",
    "        batch_size = p_t.size(0)\n",
    "        likelihood_sum = torch.zeros(batch_size)\n",
    "        k_t = torch.argmax(s_t, dim=1)\n",
    "\n",
    "        for j in range(J):\n",
    "            mean = self.means[k_t]\n",
    "            var = self.vars[k_t]\n",
    "            likelihood_sum += gaussian_log_likelihood(current_x[:, j], mean, var)\n",
    "\n",
    "        # Equation 13\n",
    "        entropy = -torch.sum(p_t * torch.log(p_t), dim=1)\n",
    "\n",
    "        # Equation 14\n",
    "        beta_e = C_1 if e < E_1 else 0.0\n",
    "\n",
    "        # Equation 15\n",
    "        lambda_e = lambda_1 * np.exp((p * e) / E_2 - p) if e < E_2 else lambda_1\n",
    "\n",
    "        self.lambda_epochs[e] = lambda_e\n",
    "\n",
    "        # Equation 12\n",
    "        jump_loss = lambda_e * torch.norm(p_t - p_prev, p=1, dim=1)\n",
    "        return torch.mean(-likelihood_sum / J - beta_e * entropy + jump_loss)\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for time series data with sliding windows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, window_size: int):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = max(0, idx - self.window_size)\n",
    "        return torch.FloatTensor(self.data[start:idx])\n",
    "\n",
    "\n",
    "def train_grstu(data: np.ndarray):\n",
    "    eval_data = torch.FloatTensor(data)\n",
    "    dataset = TimeSeriesDataset(data, L)\n",
    "    #dataloader = DataLoader(dataset, batch_size=B, shuffle=True, drop_last=True)\n",
    "\n",
    "    model = GRSTU()\n",
    "    loss_fn = GRSTULoss(data)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=nu)\n",
    "\n",
    "    # Training variables\n",
    "    best_loss = np.inf\n",
    "    best_states = None\n",
    "    best_means = None\n",
    "    best_vars = None\n",
    "    avg_losses = []\n",
    "\n",
    "    training_loop = tqdm(range(epochs))\n",
    "\n",
    "    for e in training_loop:\n",
    "        model.train()\n",
    "\n",
    "        batch_losses = []\n",
    "        current_states = []\n",
    "        prev_probs = None\n",
    "\n",
    "        model.init_hidden(1)\n",
    "\n",
    "        for t in range(L + 1, len(data)):\n",
    "            x_batch = torch.FloatTensor(data[t - L:t]).reshape(1, -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            probs, states = model(x_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(e, x_batch, probs, states, prev_probs)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            prev_probs = probs.detach()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "            current_states.append(states)\n",
    "\n",
    "            probs.detach()\n",
    "            states.detach()\n",
    "            prev_probs.detach()\n",
    "            model.hidden = torch.FloatTensor(model.hidden.data)\n",
    "\n",
    "        avg_loss = np.mean(batch_losses)\n",
    "        avg_losses.append(avg_loss)\n",
    "\n",
    "        # Update Gaussian parameters every U epochs\n",
    "        if e % U == 0:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                loss_fn.update_gaussian_params(eval_data[L + 1:], torch.cat(current_states, dim=0))\n",
    "\n",
    "                eval_prev_probs = None\n",
    "                eval_losses = []\n",
    "                eval_states = []\n",
    "                eval_dataloader = DataLoader(dataset, batch_size=B, shuffle=False, drop_last=True)\n",
    "\n",
    "                # Collect predictions for all data\n",
    "                for t in range(L + 1, len(dataset)):\n",
    "                    x_batch = dataset[t].reshape(1, -1)\n",
    "\n",
    "                    model.init_hidden(x_batch.size(0))\n",
    "\n",
    "                    # Forward pass\n",
    "                    probs, states = model(x_batch)\n",
    "\n",
    "                    loss = loss_fn(e, x_batch[:, -J:], probs, states, eval_prev_probs)\n",
    "\n",
    "                    eval_prev_probs = probs.detach()\n",
    "\n",
    "                    eval_losses.append(loss.item())\n",
    "\n",
    "                    eval_states.append(states)\n",
    "\n",
    "                # Save states prediction with the lowest evaluation loss\n",
    "                if np.mean(eval_losses) < best_loss:\n",
    "                    best_loss = np.mean(eval_losses)\n",
    "                    best_states = torch.cat(eval_states, dim=0).cpu().numpy()\n",
    "                    best_means = loss_fn.means.clone().cpu().numpy()\n",
    "                    best_vars = loss_fn.vars.clone().cpu().numpy()\n",
    "\n",
    "        training_loop.set_postfix(loss=avg_loss)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_states': best_states,\n",
    "        'means': best_means,\n",
    "        'vars': best_vars,\n",
    "        'losses': avg_losses,\n",
    "        'lambda_epochs': loss_fn.lambda_epochs.cpu().numpy(),\n",
    "    }\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "sp500 = pd.read_csv('sp500.csv')\n",
    "data = sp500['returns'].values\n",
    "\n",
    "results = train_grstu(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.set_title('Training Loss')\n",
    "ax.plot(results['losses'], label='Training Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.set_title('Jump Penalty')\n",
    "ax.plot(results['lambda_epochs'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('$\\\\lambda$(Epochs)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot_results(data, results, window_size=30):\n",
    "    regimes = np.argmax(results['best_states'], axis=1)\n",
    "\n",
    "    _, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Adjust time index to account for window\n",
    "    time_idx = np.arange(len(data))[window_size:window_size + len(regimes)]\n",
    "    plot_data = data[window_size:window_size + len(regimes)]\n",
    "\n",
    "    # Plot data\n",
    "    ax1.plot(time_idx, plot_data, label='Data')\n",
    "    ax2.plot(time_idx, regimes, label='Regime', color='r')\n",
    "\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax2.set_ylabel('Regime')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_results(data, results, window_size=L)"
   ],
   "id": "c675db84c30869ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11afa666cf3c15ee",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def plot_regimes(dates, prices, returns, regime_labels, means, vars):\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "    ax1.plot(dates, prices, 'k-', alpha=0.7, label='S&P 500')\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.set_title('GRSTU Regime Detection - Prices')\n",
    "\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax1.xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "    ax2.plot(dates, returns, 'k-', alpha=0.3, label='Returns')\n",
    "    ax2.set_ylabel('Returns (%)')\n",
    "    ax2.set_xlabel('Date')\n",
    "\n",
    "    # Create a colormap for regimes\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(means)))\n",
    "\n",
    "    # Add colored backgrounds for each regime\n",
    "    for i in range(len(means)):\n",
    "        if np.any(regime_labels == i):\n",
    "            mask_indices = np.where(regime_labels == i)[0]\n",
    "            segments = np.split(mask_indices, np.where(np.diff(mask_indices) != 1)[0] + 1)\n",
    "\n",
    "            for segment in segments:\n",
    "                if len(segment) > 0:\n",
    "                    start_idx = segment[0]\n",
    "                    end_idx = segment[-1]\n",
    "\n",
    "                    # Plot on price chart\n",
    "                    ax1.axvspan(dates[start_idx], dates[end_idx], alpha=0.3, color=colors[i],\n",
    "                                label=f'Regime {i + 1}' if segment is segments[0] else \"\")\n",
    "\n",
    "                    # Plot on returns chart\n",
    "                    ax2.axvspan(dates[start_idx], dates[end_idx], alpha=0.3, color=colors[i])\n",
    "\n",
    "    # Add regime statistics to the legend\n",
    "    legend_elements = []\n",
    "    for i in range(len(means)):\n",
    "        label = f'Regime {i + 1}: μ={means[i]:.4f}, σ={np.sqrt(vars[i]):.4f}'\n",
    "        legend_elements.append(plt.Line2D([0], [0], color=colors[i], lw=4, label=label))\n",
    "\n",
    "    # Add legend to both subplots\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "    # Format x-axis\n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\"\"\n",
    "\n",
    "dates = pd.date_range(end=datetime.now(), periods=len(data)).values\n",
    "\n",
    "prices = np.zeros(len(data))\n",
    "prices[0] = 1000\n",
    "for t in range(1, len(data)):\n",
    "    prices[t] = prices[t - 1] * (1 + data[t] / 100)  # Assuming returns are in percentage\n",
    "\n",
    "# Extract results\n",
    "states = results['best_states']\n",
    "regime_labels = np.argmax(states, axis=1)\n",
    "regime_means = results['means']\n",
    "regime_vars = results['vars']\n",
    "\n",
    "# Adjust for window size (L)\n",
    "dates_adjusted = dates[L:L + len(regime_labels)]\n",
    "prices_adjusted = prices[L:L + len(regime_labels)]\n",
    "returns_adjusted = data[L:L + len(regime_labels)]\n",
    "\n",
    "known_states_adjusted = None\n",
    "\n",
    "# Plot results with dates\n",
    "plot_regimes(\n",
    "    dates_adjusted,\n",
    "    prices_adjusted,\n",
    "    returns_adjusted,\n",
    "    regime_labels,\n",
    "    regime_means,\n",
    "    regime_vars\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = np.argmax(results['best_states'], axis=1)\n",
    "y_test = sp500['states'].values[:len(y_pred)]\n",
    "\n",
    "print('Balanced Accuracy:', balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ],
   "id": "17bb782c57877b2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1115f636698e8de",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
