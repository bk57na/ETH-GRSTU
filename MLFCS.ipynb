{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "sns.set(context='paper')\n",
    "\n",
    "K = 2  # Number of hidden states\n",
    "\n",
    "# Hyperparameters for 2 states (/3 states), defined in exhibit 2\n",
    "H = 32  # GRSTU hidden state size\n",
    "grad_clip = 0.001  # Gradient clipping threshold\n",
    "B = 64  # Batch size\n",
    "R = 20  # (/80) Length of truncated backpropagation\n",
    "L = 30  # Window of past observations fed to the model\n",
    "J = 10  # Parameter for the likelihood loss\n",
    "nu = 0.005  # Learning rate\n",
    "U = 30  # Frequency at which the distributions are updated\n",
    "C_1 = 0.1\n",
    "E_1 = 500  # (/400)\n",
    "E_2 = 250  # (/250)\n",
    "epochs = 750  # Number of epochs for which the model is trained\n",
    "p = 7\n",
    "lambda_1 = 3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class STE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Returns one hot-hot encoded vector (definition of s_t in bottom of page 5)\n",
    "        one_hot = torch.zeros_like(x)\n",
    "        return one_hot.scatter_(0, x.argmax(dim=0), 1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Transparent backward pass\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "t_min = 50  # TODO define t_min\n",
    "\n",
    "\n",
    "class GRSTU(nn.Module):\n",
    "    def __init__(self, init_std=0.02):  # TODO ensure optimal value for init_std (use Xavier initialization?)\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_u = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_r = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_h = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_y = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.W_p = nn.Parameter(torch.randn(K, H) * init_std)\n",
    "        self.R_u = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.R_r = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.R_h = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "\n",
    "        self.b_u = nn.Parameter(torch.zeros(H))\n",
    "        self.b_r = nn.Parameter(torch.zeros(H))\n",
    "        self.b_h = nn.Parameter(torch.zeros(H))\n",
    "        self.b_y = nn.Parameter(torch.zeros(H))\n",
    "        self.b_p = nn.Parameter(torch.zeros(K))\n",
    "\n",
    "        # TODO how should BatchNorm be used for a 1D vector?\n",
    "        self.batch_norm = nn.Identity() #\n",
    "\n",
    "    def step(self, v_t, h_prev):\n",
    "        h_t = self.forward_pass(v_t, h_prev)\n",
    "        p_t = self.compute_state_probabilities(h_t)\n",
    "        s_t = STE.apply(p_t)\n",
    "\n",
    "        return h_t, p_t, s_t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return None  # TODO implement forward for GRSTU\n",
    "\n",
    "    def forward_pass(self, v_t, h_prev):\n",
    "        z_t = torch.sigmoid(self.W_u @ v_t + self.R_u @ h_prev + self.b_u)  # Eq (3)\n",
    "        r_t = torch.sigmoid(self.batch_norm(self.W_r @ v_t + self.R_r @ h_prev + self.b_r))  # Eq (4)\n",
    "        h_dot_t = torch.tanh(self.W_h @ v_t + self.R_h @ (r_t * h_prev) + self.b_h)  # Eq (5)\n",
    "\n",
    "        # Eq (6)\n",
    "        return (1 - z_t) * h_prev + z_t * h_dot_t  # h_t\n",
    "\n",
    "    def compute_state_probabilities(self, h_t):\n",
    "        # Eq (7)\n",
    "        return torch.softmax(self.W_p @ torch.relu(self.W_y @ h_t + self.b_y) + self.b_p, dim=0)  # p_t"
   ],
   "id": "30436dd849aa4ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_loss(x, t, e, mu_kt, sigma_sq_kt, p_t, p_prev):\n",
    "    p_norm = torch.sum(torch.abs(p_t - p_prev))\n",
    "    entropy = -torch.sum(p_t * torch.log(p_t))  # Eq (13)\n",
    "    beta_e = C_1 if e < E_1 else 0.0  # Eq (14)\n",
    "    lambda_e = lambda_1 * torch.exp((p * e) / E_2 - p) if e < E_2 else lambda_1  # Eq (15)\n",
    "\n",
    "    log_gauss = -0.5 * torch.log(2 * torch.pi * sigma_sq_kt) - 0.5 * (x[t] - mu_kt) ** 2 / sigma_sq_kt  # Eq (9)\n",
    "\n",
    "    return -1 / J * torch.sum(log_gauss - beta_e * entropy + lambda_e * p_norm)  # Eq (12)"
   ],
   "id": "d4e1aab5d47fbbcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "x = torch.tensor(pd.read_csv('simulated_hmm_500.csv')['returns_3'].values, dtype=torch.float32)\n",
    "T = len(x)\n",
    "\n",
    "# Initialize distributions (exhibit 3)\n",
    "mu = torch.full((K,), x.mean())\n",
    "sigma_square = torch.full((K,), x.var())\n",
    "\n",
    "model = GRSTU()\n",
    "optimizer = optim.Adam(model.parameters(), lr=nu)\n",
    "\n",
    "losses = []\n",
    "training_loop = tqdm(range(epochs))\n",
    "\n",
    "for e in training_loop:\n",
    "    model.train()\n",
    "\n",
    "    h_t = torch.zeros(H)\n",
    "    p_prev = torch.zeros(K)\n",
    "    k = torch.zeros(T - t_min)\n",
    "\n",
    "    for t in range(t_min, T):\n",
    "        h_t, p_t, s_t = model.step(x[t-L:t], h_t)\n",
    "        k_t = torch.argmax(s_t)\n",
    "        loss = compute_loss(x, t, torch.tensor(e), mu[k_t], sigma_square[k_t], p_t, p_prev)\n",
    "\n",
    "        k[t - t_min] = k_t\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        h_t = h_t.detach()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if e % U == 0:\n",
    "        mu = torch.tensor([(x[t_min:][k == i]).mean() for i in range(K)])  # Eq (10)\n",
    "        sigma_square = torch.tensor([(x[t_min:][k == i]).var() for i in range(K)])  # Eq (11)\n",
    "\n",
    "        for i in range(K):\n",
    "            temp_x = (x[t_min:][k == i])\n",
    "            temp_var = temp_x.var()\n",
    "\n",
    "    training_loop.set_postfix(loss=loss.item())"
   ],
   "id": "32f27475b2a6b17f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.set_title('Training Loss')\n",
    "ax.plot(losses, label='Training Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()"
   ],
   "id": "82b2696fc66cb7c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
