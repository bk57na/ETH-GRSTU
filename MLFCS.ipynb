{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "# Hyperparameters for 2 states (/3 states), defined in exhibit 2\n",
    "H = 32  # GRSTU hidden state size\n",
    "grad_clip = 0.001  # Gradient clipping threshold\n",
    "B = 64  # Batch size\n",
    "R = 20  # (/80) Length of truncated backpropagation\n",
    "L = 30  # Window of past observations fed to the model\n",
    "J = 10  # Parameter for the likelihood loss\n",
    "nu = 0.005  # Learning rate\n",
    "U = 30  # Frequency at which the distributions are updated\n",
    "C_1 = 0.1\n",
    "E_1 = 500  # (/400)\n",
    "E_2 = 250  # (/250)\n",
    "epochs = 750  # Number of epochs for which the model is trained\n",
    "p = 7\n",
    "lambda_1 = 3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class STE(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Returns one hot-hot encoded vector (definition of s_t in bottom of page 5)\n",
    "        return F.one_hot(x.argmax(dim=0), num_classes=x.size(0))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Transparent backward pass\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "K = 1  # TODO define K\n",
    "t_min = 0  # TODO define t_min\n",
    "\n",
    "\n",
    "class GRSTU(nn.Module):\n",
    "    def __init__(self, init_std=0.02):  # TODO ensure optimal value for init_std (use Xavier initialization?)\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_u = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_r = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_h = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_y = nn.Parameter(torch.randn(H, L) * init_std)\n",
    "        self.W_p = nn.Parameter(torch.randn(K, H) * init_std)\n",
    "        self.R_u = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.R_r = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "        self.R_h = nn.Parameter(torch.randn(H, H) * init_std)\n",
    "\n",
    "        self.b_u = nn.Parameter(torch.zeros(H))\n",
    "        self.b_r = nn.Parameter(torch.zeros(H))\n",
    "        self.b_h = nn.Parameter(torch.zeros(H))\n",
    "        self.b_y = nn.Parameter(torch.zeros(H))\n",
    "        self.b_p = nn.Parameter(torch.zeros(K))\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(H)\n",
    "\n",
    "    def compute_loss(self, t, e, k, k_t, p_t, p_prev):\n",
    "        mu_i = torch.mean(x[k_t == k])  # Eq (10)\n",
    "        sigma_sq_i = torch.var(x[k_t == k])  # Eq (11)\n",
    "        p_norm = torch.sum(torch.abs(p_t - p_prev))\n",
    "        entropy = -torch.sum(p_t * torch.log(p_t))  # Eq (13)\n",
    "        beta_e = C_1 if e < E_1 else 0.0  # Eq (14)\n",
    "        lambda_e = lambda_1 * torch.exp((p * e) / E_2 - p) if e < E_2 else lambda_1  # Eq (15)\n",
    "\n",
    "        log_gauss = -0.5 * torch.log(2 * torch.pi * sigma_sq_i) - 0.5 * (x[t] - mu_i) ** 2 / sigma_sq_i  # Eq (9)\n",
    "\n",
    "        return -1 / J * torch.sum(log_gauss - beta_e * entropy + lambda_e * p_norm)  # Eq (12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_t = torch.zeros(H)\n",
    "        p_prev = torch.zeros(K)\n",
    "        k = []\n",
    "\n",
    "        for t in range(t_min, T):\n",
    "            h_t = self.forward_pass(x[t - L:t], h_t)\n",
    "            p_t = self.compute_state_probabilities(h_t)\n",
    "            s_t = STE.apply(p_t)\n",
    "            k_t = torch.argmax(s_t)\n",
    "            loss = self.compute_loss(t, e, k, k_t, p_t, p_prev)\n",
    "\n",
    "            k.append(k_t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "        return None  # TODO implement forward for GRSTU\n",
    "\n",
    "    def forward_pass(self, v_t, h_prev):\n",
    "        z_t = torch.sigmoid(self.W @ v_t + self.R_u @ h_prev + self.b_u)  # Eq (3)\n",
    "        r_t = torch.sigmoid(self.batch_norm(self.W_r @ v_t + self.R_t @ h_prev + self.b_r))  # Eq (4)\n",
    "        h_dot_t = torch.tanh(self.W_h @ v_t + self.R_h @ (r_t * h_prev) + self.b_h)  # Eq (5)\n",
    "\n",
    "        # Eq (6)\n",
    "        return (1 - z_t) * h_prev + z_t * h_dot_t  # h_t\n",
    "\n",
    "    def compute_state_probabilities(self, h_t):\n",
    "        # Eq (7)\n",
    "        # TODO verify dimension for softmax?\n",
    "        return torch.softmax(self.W_p @ torch.relu(self.W_y @ h_t + self.b_y) + self.b_p, dim=0)  # p_t"
   ],
   "id": "30436dd849aa4ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.tensor([])  # Time series TODO load data\n",
    "T = len(x)\n",
    "\n",
    "mu = torch.full(K, x.mean())\n",
    "sigma_square = torch.full(K, x.var())"
   ],
   "id": "d4e1aab5d47fbbcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "\n",
    "model = GRSTU()\n",
    "optimizer = optim.Adam(model.parameters(), lr=nu)\n",
    "\n",
    "for e in range(epochs):  # TODO is E the same as Epochs? Couldn't find definition of E\n",
    "    model.train()\n",
    "\n",
    "    # TODO training loop"
   ],
   "id": "32f27475b2a6b17f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fad2c9a8f845476a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
